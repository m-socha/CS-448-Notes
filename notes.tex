\documentclass[12pt,titlepage]{article}
\usepackage[margin=1in]{geometry}
\usepackage{parskip}
\usepackage{amsmath}

\let\stdsection\section
\renewcommand\section{\clearpage\stdsection}

\usepackage{hyperref}
\hypersetup{
  linktoc=all
}

\begin{document}
  \begin{titlepage}
    \vspace*{\fill}
    \centering

    \textbf{\Huge CS 448 Course Notes} \\ [0.4em]
    \textbf{\Large Database Systems Implementation} \\ [1em]
    \textbf{\Large Michael Socha} \\ [1em]
    \textbf{\large University of Waterloo} \\
    \textbf{\large Winter 2019} \\
    \vspace*{\fill}
  \end{titlepage}

  \newpage 

  \pagenumbering{roman}

  \tableofcontents

  \newpage

  \pagenumbering{arabic}

  \section{Course Overview}
    This is a course about the inner workings of database management systems (DBMS). Key topics covered include:
    \begin{itemize}
      \item Storage systems and disk-based data structures
      \item Query processing and optimization
      \item Transactions
    \end{itemize}

  \section{Disks and Files}

    \subsection{Data Storage Devices}
      Most data in databases is stored on hard disks, which have their memory divided into pages. A random page
      can be retrieved at some fixed cost. Tapes can be used as a cheaper alternative to disks, but their data
      must be read in order.

      \subsubsection{Accessing Disk Page}
        Time to access (i.e. read/write) a disk block can be estimated by summing the following:
        \begin{itemize}
          \item \textbf{Seek time}, which is the times it take to move the disk arms to the head of the disk block.
          \item \textbf{Rotational delay}, which is the time it takes for the disk block to rotate under the head.
          \item \textbf{Transfer time}, which is the time it takes for moving data to/from the disk block.
        \end{itemize}

        Hard disks have a more complicated structure than they used to when these terms were coined, but the
        estimations provided by these terms remain fairly accurate.

        To minimize rotational delay, blocks that are fetched in quick succession should be stored close together.
        This allows for pre-fetching, where extra blocks of data are fetched in anticipation that they will be
        read shortly.

    \subsection{RAID}
      Redundant Array of Independent Disks (RAID) is a storage system organization that combines multiple physical
      disks into logical storage units. Common goals of RAID include data redundancy and parallel reads. The features
      offered by RAID are determined by the RAID level, with level 0 offering no redundancy, level 1 providing
      data mirroring (i.e. two disks storing same data), and higher levels offering further features such as bit
      or block level data striping.

    \subsection{Records and Files}
      A record is some group of fields storing information about an entity. For example, an employee record may
      store name, rank, and salary. A file stores a series of records. A record id (rid) is an identifier that can
      be used to physically locate a record in memory (e.g. page ID and position on that page).

      Common structures of files include:
      \begin{itemize}
        \item \textbf{Heap File:} These files store records in random order. They are good for write-heavy workloads,
          but not optimized for searching.
        \item \textbf{Sorted File:} These files sort their records by some key. They are good for range-based and
          equality-based lookups of records, but updates are slower than for heap files.
      \end{itemize}

    \subsection{Buffer Management}
      The buffer manager is a low-level component of a DBMS that moves pages between persistent storage (e.g. disks)
      and RAM. Data must be stored in RAM in order to be read or written to. DBMS systems typically implement a custom
      file system, since the default operating system (OS) file system may run into issues with portability, or may
      have other limitations such as not supporting files spanning multiple disks.

  \section{Indexing}
      Files can include indices, which are data structures that speed the retrieval of records searched for by an index's
      search key fields. For example, if a file stores records of employees and one wants to speed up searching by name,
      an index can be created with name as the search key. In this example, a name is considered to be a data entry, while
      the employee is considered to be a data record. Indices can also be created over multiple search keys (known as composite
      search keys). While indices can speed up reads, they tend to add overhead to writes that slows them down.

    \subsection{Data Storage in Indices}
      An index can facilitate retrieval of records by either:
      \begin{enumerate}
        \item Storing a data record next to the data entry. At most one index on a collection of data records can use
          this technique, since otherwise records are duplicated. Also, large data records may result in fewer data entries
          per page, which can result in more page fetches.
        \item Storing the rid of where the data can be retrieved. This allows more data entries to fit on a page, since (possibly
          large) data records are not stored next to the data entries. However, a separate page load is necessary to retrieve
          a data record based on its rid.
      \end{enumerate}

    \subsection{Index Classification}
      If a search key contains the primary key of a file, it is known as the primary index. Other indices are known as secondary
      indices.

      If the order of data records is the order of data entries of an index, the index is considered clustered. (e.g. see storage
      example 1 in above section). Clustered indices can greatly speed retrieving records due to a decreased number of page loads.
      To avoid duplication of records, at most one index in a file can be clustered. However, there can be multiple partially
      clustered indices, which are indices in which the order of data records is roughly that of the data entries, which can provide
      similar advantages to clustered indices. For example, if employee salary is a clustered index, employee rank may be partially
      clustered, since rank is likely correlated to salary.

    \subsection{Index Implementations}
      Indices are most commonly implemented using hash tables or tree structures.

      \subsubsection{Tree-Based Indices}
        Tree-based indices are implemented using search trees. Since search trees are ordered, tree-based indices support both equality
        and range searches.

        The most widely used search-based indices are B+ trees. These are trees that store alphabetically ordered search keys in internal
        nodes and data entries in leaf nodes. Internal nodes are typically sized so that each node is the same size as a page in memory,
        limiting the number of page loads. The number of pointers to child nodes in an internal node is known as a B+ tree's fanout. A
        large fanout decreases the tree's height.

        Tree-based structures may need to adjust their index after every write. Thus, when many writes are done in sequence it can be
        advantageous to simply sort the entries and then re-build the B+ tree's index after the writes are complete, decreasing the number
        of page loads. Such techniques for inserting multiple records efficiently are known as bulk loading.

      \subsubsection{Hash-Based Indices}
        Hash-based indices implement search using a hash table with buckets containing data entries. Hash-based indices can support equality
        searches, but since hash table are unordered, they do not speed up range searches.

        Hash-based indices differ in their hashing schemes. Some common hashing schemes are:
        \begin{itemize}
          \item \textbf{Static Hashing} uses a fixed number of buckets, and entries point to a chain of data records. This chain can grow
            long if there are many collisions.
          \item \textbf{Extendible Hashing} is a form of dynamic hashing, which can dynamically grow and shrink the number of buckets to balance
            memory usage with lowering the risk of collisions. The high-level idea is to split a bucket when it overflows, which may involve
            doubling the keyspace, and to undo this operation if a bucket becomes empty.
          \item \textbf{Linear Hashing} is another form of dynamic hashing. It avoids the ``directory-like'' structure of extendable hashing by
            allowing the number of buckets to increase one at a time instead of growing by doubling the keyspace. It supports overflow entries,
            but avoids long overflow chains by redistributing entries in a round-robin fashion when a new bucket is added.
        \end{itemize}

    \subsection{Cost Models}
      When comparing index and file organization options, it can be helpful to model the cost of common operations (search (equality and range),
      insert, update, delete) using the following parameters:
      \begin{itemize}
        \item $B$ - number of pages with data
        \item $R$ - number of data records per page
        \item $D$ - average time to read or write page
      \end{itemize}

      Index and file organization options should be considered with the database's type of workload and data in mind (i.e. workload profiling and
      data profiling).

    \subsection{Index-Only Plans}
      Some queries can be answered without actually retrieving any data records. For example, if a query is simply counting the number of employees
      with a certain name, then it is sufficient to count the number of data entries in an index using employee name as the search key; no data records
      need to be accessed, making this an index-only plan.

  \section{External Sorting}

    \subsection{Motivation}
      Sorting is a very common problem in computer science. Sample sorting applications in databases include returning sorted data, sorting intermediary
      results to speed up some other operation such as removing duplicates, and bulk loading B+ trees.

      Sorting in databases often deals with very large amounts of data that cannot all fit into RAM. This introduces the need for external sorting algorithms.

    \subsection{External Merge Sort}
      Consider a case where $N$ pages of data need to be sorted using $B$ pages of memory (known as buffer memory). The external merge sort algorithm is as
      follows:

      \begin{enumerate}
        \item Sort the values within in each page in memory. This can be done using any sort algorithm. One variant is to use heapsort to read in $B$ blocks
          right away to produce sorted runs with an average length of $2B$.
        \item Repeatedly use $B$ buffer pages to run merge sort on the merged sublists. Each run produces $\frac{N}{B}$ sorted runs, and each sorted run grows
          by $B$ pages between sorting rounds.
      \end{enumerate}

      The total number of sorting rounds (also known as passes) is around $1 + \log_{B-1}((\frac{N}{B}))$. The number of read/write page operations in each
      pass is $2N$ (i.e. one for loading page into buffer memory, one for writing sorted buffer back to persistent memory).

    \subsection{Using B+ Trees}
      Consider a case where a relation to be sorted has a B+ tree index over the sort columns. If the index is clustered, then using the index can be very
      effective, since the records can be retrieved in order page-by-page. If the index is unclustered, the order of page loading will be seemingly random
      and result in heavy page thrashing.

  \section{Evaluation of Relational Operations}

    \subsection{Joins}
      Joining data across tables is a common and potentially very expensive database operation. Various join algorithms are available and their performance
      can vary greatly depending on the joined data, the available indices, and the join conditions.

      The below examples consist of joining tables $R$ and $S$ over some condition. For cost analysis, $M$ is the number of pages in $R$, $N$ is the number
      of pages in $N$, and $p_R$ is the number of tuples per page in $R$.

      \subsubsection{Simple Nested Loop Join}
        The simple nested loop join tables $R$ and $S$ is as follows:
        \begin{enumerate}
          \item Loop through each tuple $r$ in $R$.
          \item For each $r$, loop through each tuple $s$ in $S$.
          \item If the join condition evaluates to true for $r, s$, then we add $(r, s)$ to the output.
        \end{enumerate}

        $R$ is considered to the be outer relation, and $S$ is considered to be the inner relation. When done naively by just looping through one record at a
        time, the number of IO operations is around $M + p_R M N$. The $p_R$ term can be removed by finding matches between $R$ and $S$ one page at a time
        rather than one record at a time, making the number of IO operations $M + M N$. This variant is known as page-oriented or block nested loop join.

      \subsubsection{Index Nested Loop Join}
        Index nested loop joins differ from simple nested loop joins in that they can use an index of the inner relation. The number of IO operations is
        $M + (M p_R) \text{ cost of findings matching tuples in } S$.

      \subsection{Sort Merge Join}
        Sort merge join involves sorting $R$ and $S$ on the join column. $R$ and $S$ are then scanned sequentially to look for matches. The cost of sorting
        $R$ is $M \log{M}$ and the cost of sorting $S$ is $N \log{N}$. The cost of scanning after sorting is usually around $M + N$, though could be as high
        as $M N$.

        Sort merge join can be altered so that the joining occurs during the merging portion of external merge join.

      \subsubsection{Hash Join}
        In a hash join, a hash function $h$ is used to partition relations $R$ and $S$. These partitions should be small enough to fit in main memory, and they
        are then loaded in pairs, after which matches are found between them. To further speed lookup, a hash table can be built for each partition of $R$,
        though this requires a bit more memory.

        Hash join has a cost of $2(M + N)$ during the partitioning phase, and $M + N$ during the matching phase. This is similar to the cost of sort merge join.
        Hash join is highly parallelizable and performs better than sort merge join if relation sizes vary greatly. However, sort merge join is less susceptible
        to data skew.

      \subsubsection{General Join Conditions}
        All of these algorithms work for equality-based join conditions, including multiple conjunct equality terms. Sort merge join and hash join do not work for
        inequality conditions (e.g. greater than). If using an index nested loop join for such inequality conditions, it should be clustered. If there is no clustered
        index, then the block nested loop join is likely the best option.

\end{document}
